{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Young Esposito &lt;Young@iworld.de&gt;</td>\n",
       "      <td>user4@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-05 23:31:02+00:00</td>\n",
       "      <td>Never agree to be a loser</td>\n",
       "      <td>Buck up, your troubles caused by small dimensi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mok &lt;ipline's1983@icable.ph&gt;</td>\n",
       "      <td>user2.2@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-05 23:31:03+00:00</td>\n",
       "      <td>Befriend Jenna Jameson</td>\n",
       "      <td>\\nUpgrade your sex and pleasures with these te...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daily Top 10 &lt;Karmandeep-opengevl@universalnet...</td>\n",
       "      <td>user2.9@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-06 08:28:00+00:00</td>\n",
       "      <td>CNN.com Daily Top 10</td>\n",
       "      <td>&gt;+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Parker &lt;ivqrnai@pobox.com&gt;</td>\n",
       "      <td>SpamAssassin Dev &lt;xrh@spamassassin.apache.org&gt;</td>\n",
       "      <td>2008-08-05 23:31:20+00:00</td>\n",
       "      <td>Re: svn commit: r619753 - in /spamassassin/tru...</td>\n",
       "      <td>Would anyone object to removing .so from this ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gretchen Suggs &lt;externalsep1@loanofficertool.com&gt;</td>\n",
       "      <td>user2.2@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-05 23:31:21+00:00</td>\n",
       "      <td>SpecialPricesPharmMoreinfo</td>\n",
       "      <td>\\nWelcomeFastShippingCustomerSupport\\nhttp://7...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sender  \\\n",
       "0                   Young Esposito <Young@iworld.de>   \n",
       "1                       Mok <ipline's1983@icable.ph>   \n",
       "2  Daily Top 10 <Karmandeep-opengevl@universalnet...   \n",
       "3                 Michael Parker <ivqrnai@pobox.com>   \n",
       "4  Gretchen Suggs <externalsep1@loanofficertool.com>   \n",
       "\n",
       "                                         receiver                       date  \\\n",
       "0                     user4@gvc.ceas-challenge.cc  2008-08-05 23:31:02+00:00   \n",
       "1                   user2.2@gvc.ceas-challenge.cc  2008-08-05 23:31:03+00:00   \n",
       "2                   user2.9@gvc.ceas-challenge.cc  2008-08-06 08:28:00+00:00   \n",
       "3  SpamAssassin Dev <xrh@spamassassin.apache.org>  2008-08-05 23:31:20+00:00   \n",
       "4                   user2.2@gvc.ceas-challenge.cc  2008-08-05 23:31:21+00:00   \n",
       "\n",
       "                                             subject  \\\n",
       "0                          Never agree to be a loser   \n",
       "1                             Befriend Jenna Jameson   \n",
       "2                               CNN.com Daily Top 10   \n",
       "3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
       "4                         SpecialPricesPharmMoreinfo   \n",
       "\n",
       "                                                body  label  urls  \n",
       "0  Buck up, your troubles caused by small dimensi...    1.0   1.0  \n",
       "1  \\nUpgrade your sex and pleasures with these te...    1.0   1.0  \n",
       "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...    1.0   1.0  \n",
       "3  Would anyone object to removing .so from this ...    0.0   1.0  \n",
       "4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...    1.0   1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"combined_cleaned_data_updated.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeffkhong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeffkhong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jeffkhong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Young Esposito &lt;Young@iworld.de&gt;</td>\n",
       "      <td>user4@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-05 23:31:02+00:00</td>\n",
       "      <td>Never agree to be a loser</td>\n",
       "      <td>Buck up, your troubles caused by small dimensi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Never agree to be a loser Buck up, your troubl...</td>\n",
       "      <td>never agree loser buck trouble caused small di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mok &lt;ipline's1983@icable.ph&gt;</td>\n",
       "      <td>user2.2@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-05 23:31:03+00:00</td>\n",
       "      <td>Befriend Jenna Jameson</td>\n",
       "      <td>\\nUpgrade your sex and pleasures with these te...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Befriend Jenna Jameson \\nUpgrade your sex and ...</td>\n",
       "      <td>befriend jenna jameson upgrade sex pleasure te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daily Top 10 &lt;Karmandeep-opengevl@universalnet...</td>\n",
       "      <td>user2.9@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-06 08:28:00+00:00</td>\n",
       "      <td>CNN.com Daily Top 10</td>\n",
       "      <td>&gt;+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CNN.com Daily Top 10 &gt;+=+=+=+=+=+=+=+=+=+=+=+=...</td>\n",
       "      <td>cnncom daily top daily top cnncom top video st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Parker &lt;ivqrnai@pobox.com&gt;</td>\n",
       "      <td>SpamAssassin Dev &lt;xrh@spamassassin.apache.org&gt;</td>\n",
       "      <td>2008-08-05 23:31:20+00:00</td>\n",
       "      <td>Re: svn commit: r619753 - in /spamassassin/tru...</td>\n",
       "      <td>Would anyone object to removing .so from this ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Re: svn commit: r619753 - in /spamassassin/tru...</td>\n",
       "      <td>svn commit spamassassintrunk libmailspamassass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gretchen Suggs &lt;externalsep1@loanofficertool.com&gt;</td>\n",
       "      <td>user2.2@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-05 23:31:21+00:00</td>\n",
       "      <td>SpecialPricesPharmMoreinfo</td>\n",
       "      <td>\\nWelcomeFastShippingCustomerSupport\\nhttp://7...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SpecialPricesPharmMoreinfo \\nWelcomeFastShippi...</td>\n",
       "      <td>specialpricespharmmoreinfo welcomefastshipping...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sender  \\\n",
       "0                   Young Esposito <Young@iworld.de>   \n",
       "1                       Mok <ipline's1983@icable.ph>   \n",
       "2  Daily Top 10 <Karmandeep-opengevl@universalnet...   \n",
       "3                 Michael Parker <ivqrnai@pobox.com>   \n",
       "4  Gretchen Suggs <externalsep1@loanofficertool.com>   \n",
       "\n",
       "                                         receiver                       date  \\\n",
       "0                     user4@gvc.ceas-challenge.cc  2008-08-05 23:31:02+00:00   \n",
       "1                   user2.2@gvc.ceas-challenge.cc  2008-08-05 23:31:03+00:00   \n",
       "2                   user2.9@gvc.ceas-challenge.cc  2008-08-06 08:28:00+00:00   \n",
       "3  SpamAssassin Dev <xrh@spamassassin.apache.org>  2008-08-05 23:31:20+00:00   \n",
       "4                   user2.2@gvc.ceas-challenge.cc  2008-08-05 23:31:21+00:00   \n",
       "\n",
       "                                             subject  \\\n",
       "0                          Never agree to be a loser   \n",
       "1                             Befriend Jenna Jameson   \n",
       "2                               CNN.com Daily Top 10   \n",
       "3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
       "4                         SpecialPricesPharmMoreinfo   \n",
       "\n",
       "                                                body  label  urls  \\\n",
       "0  Buck up, your troubles caused by small dimensi...    1.0   1.0   \n",
       "1  \\nUpgrade your sex and pleasures with these te...    1.0   1.0   \n",
       "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...    1.0   1.0   \n",
       "3  Would anyone object to removing .so from this ...    0.0   1.0   \n",
       "4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...    1.0   1.0   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  Never agree to be a loser Buck up, your troubl...   \n",
       "1  Befriend Jenna Jameson \\nUpgrade your sex and ...   \n",
       "2  CNN.com Daily Top 10 >+=+=+=+=+=+=+=+=+=+=+=+=...   \n",
       "3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
       "4  SpecialPricesPharmMoreinfo \\nWelcomeFastShippi...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  never agree loser buck trouble caused small di...  \n",
       "1  befriend jenna jameson upgrade sex pleasure te...  \n",
       "2  cnncom daily top daily top cnncom top video st...  \n",
       "3  svn commit spamassassintrunk libmailspamassass...  \n",
       "4  specialpricespharmmoreinfo welcomefastshipping...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to process text\n",
    "def process_text(df, columns_to_combine):\n",
    "    \"\"\"\n",
    "    This function combines specified columns into one text column and processes it by:\n",
    "    1. Lowercase conversion\n",
    "    2. Removing punctuation, URLs, and numeric values\n",
    "    3. Tokenization\n",
    "    4. Stopword removal\n",
    "    5. Lemmatization\n",
    "    \n",
    "    Parameters:\n",
    "        df: pandas DataFrame\n",
    "        columns_to_combine: list of columns to combine into a single text column\n",
    "    \n",
    "    Returns:\n",
    "        df: pandas DataFrame with a new 'processed_text' column\n",
    "    \"\"\"\n",
    "    def clean_text(text):        \n",
    "        # Remove unwanted patterns (URLs, HTML tags, special characters, digits, etc.)\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub(r'<.*?>+', '', text)\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub(r'\\n', '', text)\n",
    "        text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "        \n",
    "        # Tokenize text\n",
    "        tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "        alphabetic_tokens = [word for word in tokens if word.isalpha()]  # Keep only alphabetic words\n",
    "        \n",
    "        # Remove stopwords\n",
    "        no_stopwords = [word for word in alphabetic_tokens if word not in stop_words]\n",
    "        \n",
    "        # Lemmatize tokens\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in no_stopwords]\n",
    "        \n",
    "        # Join back into a single string\n",
    "        processed_text = ' '.join(lemmatized_words)\n",
    "        return processed_text\n",
    "    \n",
    "    # Combine specified columns into a single string column\n",
    "    df['combined_text'] = df[columns_to_combine].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "    \n",
    "    # Apply the text cleaning function to the combined column\n",
    "    df['processed_text'] = df['combined_text'].apply(clean_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Columns to combine\n",
    "columns_to_combine = ['subject', 'body']\n",
    "\n",
    "# Process text and create 'processed_text' column\n",
    "df = process_text(df, columns_to_combine)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset.reset_index(inplace=True, drop=True)\n",
    "test_dataset.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download the \"word2vec-google-news-300\" model\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def extract_tfidf_features(train, test, column_name, max_features=1000):\n",
    "    \"\"\"\n",
    "    Extract TF-IDF features from a specified column in the train and test datasets\n",
    "    and append them as a single column.\n",
    "\n",
    "    Parameters:\n",
    "        train: pandas DataFrame, the training dataset\n",
    "        test: pandas DataFrame, the test dataset\n",
    "        column_name: str, the name of the column containing text\n",
    "        max_features: int, the maximum number of features for TF-IDF\n",
    "    \n",
    "    Returns:\n",
    "        train: pandas DataFrame with TF-IDF features appended\n",
    "        test: pandas DataFrame with TF-IDF features appended\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    \n",
    "    # Fit on the training data and transform both train and test sets\n",
    "    train_tfidf = vectorizer.fit_transform(train[column_name])\n",
    "    test_tfidf = vectorizer.transform(test[column_name])\n",
    "    \n",
    "    # Convert TF-IDF matrices to dense format and store them as a single column\n",
    "    train['tfidf_features'] = list(train_tfidf.toarray())\n",
    "    test['tfidf_features'] = list(test_tfidf.toarray())\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Example usage:\n",
    "train_dataset, test_dataset = extract_tfidf_features(train_dataset, test_dataset, 'processed_text', max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126700/126700 [00:26<00:00, 4856.44it/s]\n",
      "100%|██████████| 31675/31675 [00:06<00:00, 5083.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def extract_word2vec_features(train, test, column_name, word2vec_model, vector_size=300):\n",
    "    \"\"\"\n",
    "    Extract Word2Vec features from a specified column in the train and test datasets\n",
    "    and append them as a single column.\n",
    "\n",
    "    Parameters:\n",
    "        train: pandas DataFrame, the training dataset\n",
    "        test: pandas DataFrame, the test dataset\n",
    "        column_name: str, the name of the column containing tokenized text\n",
    "        word2vec_model: Pretrained gensim Word2Vec model\n",
    "        vector_size: int, size of the word vectors (e.g., 300 for Google News model)\n",
    "\n",
    "    Returns:\n",
    "        train: pandas DataFrame with Word2Vec features appended\n",
    "        test: pandas DataFrame with Word2Vec features appended\n",
    "    \"\"\"\n",
    "    # Function to get Word2Vec features for a list of words\n",
    "    def get_word2vec_features(tokenized_text):\n",
    "        word_vectors = [word2vec_model[word] for word in tokenized_text if word in word2vec_model]\n",
    "        if len(word_vectors) > 0:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(vector_size)\n",
    "\n",
    "    tqdm.pandas()  # Enable tqdm for pandas\n",
    "    \n",
    "    # Apply the function to train and test datasets\n",
    "    train_word2vec = train[column_name].progress_apply(lambda x: get_word2vec_features(x.split()))\n",
    "    test_word2vec = test[column_name].progress_apply(lambda x: get_word2vec_features(x.split()))\n",
    "    \n",
    "    # Store the feature arrays in a single column\n",
    "    train['word2vec_features'] = list(np.vstack(train_word2vec))\n",
    "    test['word2vec_features'] = list(np.vstack(test_word2vec))\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Example usage:\n",
    "train_dataset, test_dataset = extract_word2vec_features(train_dataset, test_dataset, 'processed_text', word2vec_model, vector_size=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126700/126700 [1:26:01<00:00, 24.55it/s]\n",
      "100%|██████████| 31675/31675 [20:02<00:00, 26.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def extract_transformer_features(train, test, column_name, model):\n",
    "    \"\"\"\n",
    "    Extract transformer-based features from a specified column in the train and test datasets\n",
    "    and append them as a single column.\n",
    "\n",
    "    Parameters:\n",
    "        train: pandas DataFrame, the training dataset\n",
    "        test: pandas DataFrame, the test dataset\n",
    "        column_name: str, the name of the column containing text\n",
    "        model: Pretrained transformer model (e.g., SentenceTransformer)\n",
    "    \n",
    "    Returns:\n",
    "        train: pandas DataFrame with transformer features appended\n",
    "        test: pandas DataFrame with transformer features appended\n",
    "    \"\"\"\n",
    "    # Function to generate transformer-based features\n",
    "    def get_transformer_features(text):\n",
    "        return model.encode(text).tolist()\n",
    "    \n",
    "    tqdm.pandas()  # Enable tqdm for pandas\n",
    "    \n",
    "    # Apply the function to train and test datasets\n",
    "    train_transformer = train[column_name].progress_apply(get_transformer_features)\n",
    "    test_transformer = test[column_name].progress_apply(get_transformer_features)\n",
    "    \n",
    "    # Store the feature arrays in a single column\n",
    "    train['transformer_features'] = list(train_transformer)\n",
    "    test['transformer_features'] = list(test_transformer)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Example usage:\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Load the pre-trained model\n",
    "train_dataset, test_dataset = extract_transformer_features(train_dataset, test_dataset, 'processed_text', model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features from Sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract one-hot encoding features from the domain\n",
    "def extract_features_from_domain(domain):\n",
    "    # Define the top 30 domains and top 10 phishing domains\n",
    "    top30Domains = ['ENRON.com', 'gmail.com', 'samba.org', 'enron.com', 'yahoo.com', 'hotmail.com', 'unknown', \n",
    "                    'psy1.psych.arizona.edu', 'aol.com', 'mail.cnn.com', 'broadcast.shareholder.com', 'cbsig.com', \n",
    "                    'msn.com', 'yahoo.co.kr', 'python.org', 'issues.apache.org', 'messaging.accuweather.com', \n",
    "                    'earthlink.net', 'parrotcode.org', 'ccomad3.uu.commissioner.com', 'media.mit.edu', 'google.com', \n",
    "                    'foxnews.com', 'ENRON.net', 'stats.ox.ac.uk', 'v.loewis.de', 'perl.org', 'verizon.net', \n",
    "                    'flax9.uwaterloo.ca', 'gmx.net']\n",
    "\n",
    "    top10Domains_with_1_phishing = ['yahoo.co.kr', 'flax9.uwaterloo.ca', 'korea.com', 'access-one.com', 'lingo.com',\n",
    "                                    'absolutemotion.com', 'rr.com', 'nokia.com', 'daum.net', 'ebay.com']\n",
    "\n",
    "    # Initialize a feature vector of zeros for both domain lists\n",
    "    features = np.zeros(len(top30Domains) + len(top10Domains_with_1_phishing))\n",
    "\n",
    "    # One-hot encode for the top 30 domains\n",
    "    if domain in top30Domains:\n",
    "        features[top30Domains.index(domain)] = 1\n",
    "\n",
    "    # One-hot encode for the top 10 phishing domains with a phishing rate of 1.0\n",
    "    if domain in top10Domains_with_1_phishing:\n",
    "        features[len(top30Domains) + top10Domains_with_1_phishing.index(domain)] = 1\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_features_from_TLD(tld):\n",
    "    top10tld_with_less_than25_more_than75 = ['org', 'edu', 'unknown', 'kr', 'nz', 'jp', 'ru', 'gov', 'pl', 'EDU']\n",
    "    features = np.zeros(len(top10tld_with_less_than25_more_than75 ))\n",
    "    # One-hot encode for the top 30 domains\n",
    "    if tld in top10tld_with_less_than25_more_than75:\n",
    "        features[top10tld_with_less_than25_more_than75.index(tld)] = 1\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_features_from_displayName(displayName):\n",
    "    # Lists of safe and risky display names\n",
    "    display_name_safe = ['unknown', 'Schedule Crawler', '\"j.c.f.\"', '\"Jonathan C. Forster\"', '\"AccuWeather.com Alert\"', \n",
    "                         '\"Nelson, Michelle\"', '\"Commissioner.COM\"', 'Chas Owens', 'Prof Brian Ripley', 'Guido van Rossum']\n",
    "\n",
    "    display_name_risky = ['Daily Top 10', 'CNN Alerts', '\"Vanessa J. Smith\"', '\"Richard K. Lee\"', 'SCC', '\"Paul A. Davis\"', \n",
    "                          'Sydney Car Centre', 'Aegis Capital Group LLC', 'Aegis Capital Group', '']\n",
    "    \n",
    "    features_safe = np.zeros(len(display_name_safe))\n",
    "    features_risky = np.zeros(len(display_name_risky))\n",
    "    \n",
    "    # One-hot encode for display_name_safe\n",
    "    if displayName in display_name_safe:\n",
    "        features_safe[display_name_safe.index(displayName)] = 1\n",
    "    \n",
    "    # One-hot encode for display_name_risky\n",
    "    if displayName in display_name_risky:\n",
    "        features_risky[display_name_risky.index(displayName)] = 1\n",
    "\n",
    "    return np.concatenate([features_safe, features_risky])\n",
    "\n",
    "def extract_features_from_localPart(localPart):\n",
    "    # Lists of safe and risky local parts\n",
    "    local_part_safe = ['pete.davis', 'jforster', 'alert', 'tridge', 'jerry', 'qydlqcws-iacfym', 'noreply', 'cnnalerts', 'metze', 'inbox']\n",
    "\n",
    "    local_part_risky = ['>', 'Montague', 'gnitpick', 'return', 'the00', 'sales', 'adwords-noreply', 'MAILER-DAEMON', 'alerts', 'response']\n",
    "    \n",
    "    # Initialize feature vector for safe and risky local parts\n",
    "    features_safe = np.zeros(len(local_part_safe))\n",
    "    features_risky = np.zeros(len(local_part_risky))\n",
    "    \n",
    "    # One-hot encode for local_part_safe\n",
    "    if localPart in local_part_safe:\n",
    "        features_safe[local_part_safe.index(localPart)] = 1\n",
    "    \n",
    "    # One-hot encode for local_part_risky\n",
    "    if localPart in local_part_risky:\n",
    "        features_risky[local_part_risky.index(localPart)] = 1\n",
    "\n",
    "    # Combine both feature vectors (concatenated safe and risky encodings)\n",
    "    return np.concatenate([features_safe, features_risky])\n",
    "\n",
    "def extract_features_from_sender(sender):\n",
    "    # Split the sender into display name and email address\n",
    "    try:\n",
    "        displayName, email = sender.split('<')\n",
    "        displayName = displayName.strip()  # Remove whitespace\n",
    "        email = email.strip('> ')  # Remove the closing bracket and spaces\n",
    "    except ValueError:\n",
    "        # In case the sender does not contain a display name\n",
    "        displayName = 'unknown'\n",
    "        email = sender.strip()\n",
    "\n",
    "    if '@' in email:\n",
    "        localPart, domain = email.split('@')\n",
    "    else:\n",
    "        localPart, domain = '', ''  # Default to empty if email part is missing\n",
    "\n",
    "    # Extract features\n",
    "    displayName_features = extract_features_from_displayName(displayName)\n",
    "    localPart_features = extract_features_from_localPart(localPart)\n",
    "    domain_features = extract_features_from_domain(domain)\n",
    "    tld_features = extract_features_from_TLD(domain.split('.')[-1])\n",
    "\n",
    "    # Combine all features into a single feature vector\n",
    "    combined_features = np.concatenate([displayName_features, localPart_features, domain_features, tld_features])\n",
    "\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "train_dataset['sender_features'] = train_dataset['sender'].apply(extract_features_from_sender)\n",
    "test_dataset['sender_features'] = test_dataset['sender'].apply(extract_features_from_sender)\n",
    "\n",
    "# Example usage of extract_features_from_sender for a given sender\n",
    "example_sender= 'Daily Top 10 <arabia1990@mpdadvies.nl>'\n",
    "feature = extract_features_from_sender(example_sender)\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from the subject\n",
    "def extract_features_from_subject(subject):\n",
    "    # Define the blacklist and whitelist words\n",
    "    blacklist_words = [\n",
    "        'price', 'immediate', 'sex', 'replica', 'watches', 'custom', \n",
    "        'money', 'account', 'urgent', 'online', 'secure', 'penis', \n",
    "        'cnn.com'\n",
    "    ]\n",
    "    whitelist_words = [\n",
    "        'AccuWeather', 'UAI', 'perl', 'svn', 'commit', 'samba'\n",
    "    ]\n",
    "\n",
    "    # Initialize a feature vector of zeros for both word lists + 2 additional features\n",
    "    features = np.zeros(len(blacklist_words) + len(whitelist_words) + 2)\n",
    "\n",
    "    # Check for presence of blacklist words\n",
    "    for i, word in enumerate(blacklist_words):\n",
    "        if word in subject.lower():  # Check in lowercase for case insensitivity\n",
    "            features[i] = 1\n",
    "\n",
    "    # Check for presence of whitelist words\n",
    "    for i, word in enumerate(whitelist_words):\n",
    "        if word in subject.lower():  # Check in lowercase for case insensitivity\n",
    "            features[len(blacklist_words) + i] = 1\n",
    "\n",
    "    # Check if the subject is a reply\n",
    "    features[len(blacklist_words) + len(whitelist_words)] = int(subject.lower().startswith(\"re:\"))\n",
    "\n",
    "    # Check for joined words using a regular expression\n",
    "    features[len(blacklist_words) + len(whitelist_words) + 1] = int(bool(re.search(r'[a-zA-Z]+[A-Z][a-zA-Z]|[A-Z]{2,}', subject)))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "train_dataset['subject_features'] = train_dataset['subject'].apply(extract_features_from_subject)\n",
    "test_dataset['subject_features'] = test_dataset['subject'].apply(extract_features_from_subject)\n",
    "\n",
    "# Example usage of extract_features_from_subject for a given subject\n",
    "example_subject = 'Re: [samba] svn commit: r13784 - in branches/SAMBA_4_0/source: . lib/tls'\n",
    "subject_feature = extract_features_from_subject(example_subject)\n",
    "print(subject_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features for Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_month_feature(date):\n",
    "    # Extract the month directly from the string (format: 'YYYY-MM-DD')\n",
    "    month = int(date[5:7])  # Get the month as an integer\n",
    "    \n",
    "    # One-hot encode the month (for months 1-12)\n",
    "    month_dummies = [0] * 12  # Create a list of 12 zeros\n",
    "    if month > 1:  # Drop the first month (January) for one-hot encoding\n",
    "        month_dummies[month - 1] = 1  # Set the respective month index to 1\n",
    "        \n",
    "    return month_dummies  # Returns a list with one-hot encoded month\n",
    "\n",
    "def extract_features_from_date(date_with_time):\n",
    "    # Split the date and time, and extract only the date part\n",
    "    return create_month_feature(date_with_time.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_dataset['date_features'] = train_dataset['date'].apply(extract_features_from_date)\n",
    "test_dataset['date_features'] = test_dataset['date'].apply(extract_features_from_date)\n",
    "\n",
    "# Example usage of extract_features_from_date for a given date\n",
    "date_input = '2008-08-05 23:31:03+00:00'\n",
    "month_feature = create_month_feature(date_input)\n",
    "print(month_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features from Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify URLs as legitimate, suspicious, or phishing\n",
    "def classify_url(body):\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    urls = url_pattern.findall(body)\n",
    "    avg_length = 57.0  # Average URL length from the dataset\n",
    "    if not urls:\n",
    "        return [1.0, 0.0, 0.0]  # No URLs means it's legitimate\n",
    "\n",
    "    for url in urls:\n",
    "        url_length = len(url)\n",
    "        if url_length >= avg_length * 1.5:\n",
    "            return [0.0, 0.0, 1.0]  # Phishing URL\n",
    "        elif avg_length <= url_length < avg_length * 1.5:\n",
    "            return [0.0, 1.0, 0.0]  # Suspicious URL\n",
    "\n",
    "    return [1.0, 0.0, 0.0]  # Legitimate URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_redirects_binary(body):\n",
    "    \"\"\"\n",
    "    Count the number of redirects in the given HTML body and classify as binary.\n",
    "\n",
    "    Args:\n",
    "        body (str): The HTML body to check for redirects.\n",
    "\n",
    "    Returns:\n",
    "        int: 1 if phishing (at least one redirect), 0 if legitimate (no redirects).\n",
    "    \"\"\"\n",
    "    redirect_pattern = r'(?i)\\b(redirect|redirecting|window\\.location|setTimeout|location\\.href|document\\.location|location\\.replace|location\\.assign|window\\.location\\.href|meta\\s+http-equiv=[\"\\']refresh[\"\\']|<iframe|<form|<script)\\b'\n",
    "    number_of_redirects = len(re.findall(redirect_pattern, body))\n",
    "    return 1 if number_of_redirects > 0 else 0  # 1 for phishing, 0 for legitimate\n",
    "\n",
    "def count_newlines(body):\n",
    "    \"\"\"\n",
    "    Count the number of newlines in the body.\n",
    "\n",
    "    Args:\n",
    "        body (str): The HTML body to analyze.\n",
    "\n",
    "    Returns:\n",
    "        int: The count of newlines.\n",
    "    \"\"\"\n",
    "    return body.count('\\n')\n",
    "\n",
    "def transform_to_embedding(body):\n",
    "    \"\"\"\n",
    "    Generate a sentence embedding from the body using a pre-trained transformer model.\n",
    "\n",
    "    Args:\n",
    "        body (str): The text to transform into an embedding.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of floats representing the sentence embedding.\n",
    "    \"\"\"\n",
    "    return model.encode(body).tolist()\n",
    "\n",
    "# Function to extract phishing features from the email body\n",
    "def extract_phishing_features_from_text(body):\n",
    "    \"\"\"\n",
    "    Extracts phishing-related features from a single body of text.\n",
    "    \n",
    "    Args:\n",
    "        body (str): The body of the email or text to analyze.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted features.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # Define a pattern for IP addresses\n",
    "    ip_pattern = r'\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b'\n",
    "    features['ip_url_count'] = len(re.findall(ip_pattern, body))\n",
    "\n",
    "    # Define suspicious TLDs\n",
    "    suspicious_tlds = ['.xyz', '.biz', '.info']\n",
    "    features['suspicious_tld_count'] = sum(tld in body.lower() for tld in suspicious_tlds)\n",
    "\n",
    "    # Text complexity features\n",
    "    words = body.split()\n",
    "    features['word_count'] = len(words)\n",
    "    features['average_word_length'] = sum(len(word) for word in words) / len(words) if len(words) > 0 else 0\n",
    "    features['uppercase_word_count'] = sum(1 for word in words if word.isupper())\n",
    "\n",
    "    # Special characters\n",
    "    special_chars = \"!$#@\"\n",
    "    features['special_char_ratio'] = sum(1 for char in body if char in special_chars) / len(body) if len(body) > 0 else 0\n",
    "\n",
    "    # Exclamation and question marks\n",
    "    features['exclamation_count'] = body.count('!')\n",
    "    features['question_mark_count'] = body.count('?')\n",
    "\n",
    "    # HTML tags\n",
    "    features['html_tags'] = 1 if '<' in body and '>' in body else 0\n",
    "\n",
    "    # Predetermined phishing keywords\n",
    "    phishing_keywords = ['urgent', 'free', 'winner', 'offer', 'limited', 'click', 'claim', 'prize']\n",
    "    features['phishing_keyword_count'] = sum(word in body.lower() for word in phishing_keywords)\n",
    "\n",
    "    return list(features.values())\n",
    "\n",
    "# Extract features from the email body \n",
    "def extract_features_from_body(body):\n",
    "    \"\"\"\n",
    "    Extract various features from the HTML body for phishing detection.\n",
    "\n",
    "    Args:\n",
    "        body (str): The HTML body to analyze.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Count of redirects\n",
    "    features.append(classify_redirects_binary(body))\n",
    "\n",
    "    # Count of newlines\n",
    "    features.append(count_newlines(body))\n",
    "\n",
    "    # URL classification\n",
    "    url_classification = classify_url(body) \n",
    "    features.extend(url_classification)  # Append the URL classification to features\n",
    "\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "train_dataset['body_features'] = train_dataset['body'].apply(extract_features_from_body)\n",
    "test_dataset['body_features'] = test_dataset['body'].apply(extract_features_from_body)\n",
    "\n",
    "# Example usage of extract_features_from_body for a given body\n",
    "example_body = 'I hate this game, come buy bitcoin.'\n",
    "body_feature = extract_features_from_body(example_body)\n",
    "print(body_feature) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Body features 2 to contains features from extract_phishing_features_from_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 7, 4.142857142857143, 1, 0.0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_dataset['body_features_2'] = train_dataset['body'].apply(extract_phishing_features_from_text)\n",
    "test_dataset['body_features_2'] = test_dataset['body'].apply(extract_phishing_features_from_text)\n",
    "\n",
    "# Example usage of extract_phishing_features_from_text for a given body\n",
    "example_body = 'I hate this game, come buy bitcoin.'\n",
    "body_feature_2 = extract_phishing_features_from_text(example_body)\n",
    "print(body_feature_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original columns \n",
    "cols_to_drop = ['sender', 'receiver', 'date', 'subject', 'body', 'urls']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save train and test datasets to be used for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126700, 17)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31675, 17)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sender', 'receiver', 'date', 'subject', 'body', 'label', 'urls',\n",
       "       'combined_text', 'processed_text', 'tfidf_features',\n",
       "       'word2vec_features', 'transformer_features', 'sender_features',\n",
       "       'subject_features', 'date_features', 'body_features',\n",
       "       'body_features_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>tfidf_features</th>\n",
       "      <th>word2vec_features</th>\n",
       "      <th>transformer_features</th>\n",
       "      <th>sender_features</th>\n",
       "      <th>subject_features</th>\n",
       "      <th>date_features</th>\n",
       "      <th>body_features</th>\n",
       "      <th>body_features_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eleanor foster &lt;aceriwen@velnet.co.uk&gt;</td>\n",
       "      <td>Nicolette Hudson &lt;holden.salisbury@enron.com&gt;</td>\n",
       "      <td>2001-08-22 07:27:04+00:00</td>\n",
       "      <td>Male muscle boosting system</td>\n",
       "      <td>\"I've been using your product for 4 months now...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Male muscle boosting system \"I've been using y...</td>\n",
       "      <td>male muscle boosting system ive using product ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.00971660204231739, 0.06431159377098083, 0.0...</td>\n",
       "      <td>[-0.06228220835328102, -0.00879708118736744, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0, 0, 187, 4.540106951871658, 3, 0.0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lana Moore &lt;Lana.Moore@nesanet.org&gt;</td>\n",
       "      <td>'NESA Members' &lt;eva.pollard@nesanet.org&gt;</td>\n",
       "      <td>2001-10-23 15:04:52+00:00</td>\n",
       "      <td>NESA NYC Mixer - November 8</td>\n",
       "      <td>Please make plans to attend NESA's Membership ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NESA NYC Mixer - November 8 Please make plans ...</td>\n",
       "      <td>nesa nyc mixer november please make plan atten...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.04451704025268555, 0.0018677711486816406, ...</td>\n",
       "      <td>[-0.019170133396983147, -0.06289736181497574, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0, 0, 64, 4.53125, 1, 0.0028169014084507044, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Common Dreams &lt;listreply@commondreams.org&gt;</td>\n",
       "      <td>ktwarwic@flax9.uwaterloo.ca</td>\n",
       "      <td>2007-05-15 05:49:53+00:00</td>\n",
       "      <td>News &amp; Views | 05.14.07</td>\n",
       "      <td>\\n \\n\\nCommon Dreams - Breaking News &amp; Views f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>News &amp; Views | 05.14.07 \\n \\n\\nCommon Dreams -...</td>\n",
       "      <td>news view common dream breaking news view prog...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.005526355933398008, 0.04494216665625572, -...</td>\n",
       "      <td>[0.07190383970737457, -0.04377426207065582, 0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 84, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0, 0, 220, 10.027272727272727, 4, 0.000811688...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Adam H. Kerman\" &lt;ahk@chinet.chinet.com&gt;</td>\n",
       "      <td>Pine Discussion Forum &lt;pine-info@u.washington....</td>\n",
       "      <td>1998-01-08 07:10:16+00:00</td>\n",
       "      <td>Re: folder index - sender or recipient</td>\n",
       "      <td>&gt;From: Jakob Kellner  &gt;Date: Thu, 8 Jan 1998 0...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Re: folder index - sender or recipient &gt;From: ...</td>\n",
       "      <td>folder index sender recipient jakob kellner da...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0719418004155159, 0.00951385498046875, 0.02...</td>\n",
       "      <td>[0.032333265990018845, -0.03589319437742233, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0, 0, 86, 4.686046511627907, 1, 0.0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carl Helms &lt;mxenitfrnpfkue@onlinetx.net&gt;</td>\n",
       "      <td>opt4@flax9.uwaterloo.ca</td>\n",
       "      <td>2007-07-01 09:07:59+00:00</td>\n",
       "      <td>This is for you</td>\n",
       "      <td>\\nThis is amazing stuff...\\n\\nAdd some inches ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is for you \\nThis is amazing stuff...\\n\\n...</td>\n",
       "      <td>amazing stuffadd inch fast safe effective seen...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.0525309257209301, -0.00717841275036335, 0....</td>\n",
       "      <td>[-0.08619339019060135, 0.05992841348052025, -0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 11, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0, 0, 30, 5.433333333333334, 1, 0.0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sender  \\\n",
       "0      eleanor foster <aceriwen@velnet.co.uk>   \n",
       "1         Lana Moore <Lana.Moore@nesanet.org>   \n",
       "2  Common Dreams <listreply@commondreams.org>   \n",
       "3    \"Adam H. Kerman\" <ahk@chinet.chinet.com>   \n",
       "4    Carl Helms <mxenitfrnpfkue@onlinetx.net>   \n",
       "\n",
       "                                            receiver  \\\n",
       "0      Nicolette Hudson <holden.salisbury@enron.com>   \n",
       "1           'NESA Members' <eva.pollard@nesanet.org>   \n",
       "2                        ktwarwic@flax9.uwaterloo.ca   \n",
       "3  Pine Discussion Forum <pine-info@u.washington....   \n",
       "4                            opt4@flax9.uwaterloo.ca   \n",
       "\n",
       "                        date                                 subject  \\\n",
       "0  2001-08-22 07:27:04+00:00             Male muscle boosting system   \n",
       "1  2001-10-23 15:04:52+00:00             NESA NYC Mixer - November 8   \n",
       "2  2007-05-15 05:49:53+00:00                 News & Views | 05.14.07   \n",
       "3  1998-01-08 07:10:16+00:00  Re: folder index - sender or recipient   \n",
       "4  2007-07-01 09:07:59+00:00                         This is for you   \n",
       "\n",
       "                                                body  label  urls  \\\n",
       "0  \"I've been using your product for 4 months now...    1.0   1.0   \n",
       "1  Please make plans to attend NESA's Membership ...    0.0   0.0   \n",
       "2  \\n \\n\\nCommon Dreams - Breaking News & Views f...    0.0   1.0   \n",
       "3  >From: Jakob Kellner  >Date: Thu, 8 Jan 1998 0...    0.0   0.0   \n",
       "4  \\nThis is amazing stuff...\\n\\nAdd some inches ...    1.0   0.0   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  Male muscle boosting system \"I've been using y...   \n",
       "1  NESA NYC Mixer - November 8 Please make plans ...   \n",
       "2  News & Views | 05.14.07 \\n \\n\\nCommon Dreams -...   \n",
       "3  Re: folder index - sender or recipient >From: ...   \n",
       "4  This is for you \\nThis is amazing stuff...\\n\\n...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  male muscle boosting system ive using product ...   \n",
       "1  nesa nyc mixer november please make plan atten...   \n",
       "2  news view common dream breaking news view prog...   \n",
       "3  folder index sender recipient jakob kellner da...   \n",
       "4  amazing stuffadd inch fast safe effective seen...   \n",
       "\n",
       "                                      tfidf_features  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                   word2vec_features  \\\n",
       "0  [0.00971660204231739, 0.06431159377098083, 0.0...   \n",
       "1  [-0.04451704025268555, 0.0018677711486816406, ...   \n",
       "2  [-0.005526355933398008, 0.04494216665625572, -...   \n",
       "3  [0.0719418004155159, 0.00951385498046875, 0.02...   \n",
       "4  [-0.0525309257209301, -0.00717841275036335, 0....   \n",
       "\n",
       "                                transformer_features  \\\n",
       "0  [-0.06228220835328102, -0.00879708118736744, 0...   \n",
       "1  [-0.019170133396983147, -0.06289736181497574, ...   \n",
       "2  [0.07190383970737457, -0.04377426207065582, 0....   \n",
       "3  [0.032333265990018845, -0.03589319437742233, 0...   \n",
       "4  [-0.08619339019060135, 0.05992841348052025, -0...   \n",
       "\n",
       "                                     sender_features  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                    subject_features  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                          date_features           body_features  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]   [0, 0, 1.0, 0.0, 0.0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]   [0, 0, 1.0, 0.0, 0.0]   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  [0, 84, 1.0, 0.0, 0.0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   [0, 0, 1.0, 0.0, 0.0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  [0, 11, 1.0, 0.0, 0.0]   \n",
       "\n",
       "                                     body_features_2  \n",
       "0  [0, 0, 187, 4.540106951871658, 3, 0.0, 0, 0, 0...  \n",
       "1  [0, 0, 64, 4.53125, 1, 0.0028169014084507044, ...  \n",
       "2  [0, 0, 220, 10.027272727272727, 4, 0.000811688...  \n",
       "3  [0, 0, 86, 4.686046511627907, 1, 0.0, 0, 1, 0, 0]  \n",
       "4  [0, 0, 30, 5.433333333333334, 1, 0.0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>tfidf_features</th>\n",
       "      <th>word2vec_features</th>\n",
       "      <th>transformer_features</th>\n",
       "      <th>sender_features</th>\n",
       "      <th>subject_features</th>\n",
       "      <th>date_features</th>\n",
       "      <th>body_features</th>\n",
       "      <th>body_features_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Josiah Rivera &lt;Gary.Washington@lambertpartners...</td>\n",
       "      <td>ktwarwic@flax9.uwaterloo.ca</td>\n",
       "      <td>2007-06-27 23:16:20+00:00</td>\n",
       "      <td>regalto</td>\n",
       "      <td>Men's Products' Discounts!!! 80% off SALE !!! ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>regalto Men's Products' Discounts!!! 80% off S...</td>\n",
       "      <td>regalto men product discount sale vaqra pill o...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.01425081118941307, 0.03438971936702728, 0.0...</td>\n",
       "      <td>[-0.09238181263208389, 0.037367917597293854, -...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 25, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0, 0, 181, 4.983425414364641, 8, 0.0109190172...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daily Top 10 &lt;bhargav-buchhand@internetwebtool...</td>\n",
       "      <td>user7-ext4@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-07-31 12:34:32+00:00</td>\n",
       "      <td>CNN.com Daily Top 10</td>\n",
       "      <td>&gt;+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CNN.com Daily Top 10 &gt;+=+=+=+=+=+=+=+=+=+=+=+=...</td>\n",
       "      <td>cnncom daily top daily top cnncom top video st...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0004533067112788558, 0.03454103693366051, -...</td>\n",
       "      <td>[-0.049507372081279755, -0.06389272958040237, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0, 0, 293, 11.542662116040956, 100, 0.0002550...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexis Kimball &lt;skeweringbaton@saveourplanet.org&gt;</td>\n",
       "      <td>mailn@flax9.uwaterloo.ca</td>\n",
       "      <td>2006-05-20 18:06:25+00:00</td>\n",
       "      <td>Don't waste your time. Cheap pills are here.</td>\n",
       "      <td>Dear customer.Wanna know how to save much on y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Don't waste your time. Cheap pills are here. D...</td>\n",
       "      <td>dont waste time cheap pill dear customerwanna ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0284550990909338, 0.02321099303662777, -0.0...</td>\n",
       "      <td>[0.021192213520407677, 0.030631644651293755, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 3, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0, 0, 95, 5.252631578947368, 0, 0.0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carmela Rios &lt;hfjdesmond@goldgeneration.com&gt;</td>\n",
       "      <td>ktwarwic &lt;ktwarwic@speedy.uwaterloo.ca&gt;</td>\n",
       "      <td>1999-10-01 00:29:22+00:00</td>\n",
       "      <td>her barbour many champion</td>\n",
       "      <td>\\n\\n\\nThis one will explode!\\n\\n\\n\\nTarget sym...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>her barbour many champion \\n\\n\\nThis one will ...</td>\n",
       "      <td>barbour many champion one explodetarget sym bv...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.036815643310546875, 0.01973724365234375, 0....</td>\n",
       "      <td>[-0.03742770478129387, -0.06625784933567047, -...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 15, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0, 0, 33, 5.363636363636363, 4, 0.05882352941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daily Top 10 &lt;sluc1977@34direct.net&gt;</td>\n",
       "      <td>email265@gvc.ceas-challenge.cc</td>\n",
       "      <td>2008-08-06 20:56:29+00:00</td>\n",
       "      <td>CNN.com Daily Top 10</td>\n",
       "      <td>&gt;+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CNN.com Daily Top 10 &gt;+=+=+=+=+=+=+=+=+=+=+=+=...</td>\n",
       "      <td>cnncom daily top daily top cnncom top video st...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0004533067112788558, 0.03454103693366051, -...</td>\n",
       "      <td>[-0.049507372081279755, -0.06389272958040237, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0, 0, 293, 11.46075085324232, 100, 0.00025660...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sender  \\\n",
       "0  Josiah Rivera <Gary.Washington@lambertpartners...   \n",
       "1  Daily Top 10 <bhargav-buchhand@internetwebtool...   \n",
       "2  Alexis Kimball <skeweringbaton@saveourplanet.org>   \n",
       "3       Carmela Rios <hfjdesmond@goldgeneration.com>   \n",
       "4               Daily Top 10 <sluc1977@34direct.net>   \n",
       "\n",
       "                                  receiver                       date  \\\n",
       "0              ktwarwic@flax9.uwaterloo.ca  2007-06-27 23:16:20+00:00   \n",
       "1         user7-ext4@gvc.ceas-challenge.cc  2008-07-31 12:34:32+00:00   \n",
       "2                 mailn@flax9.uwaterloo.ca  2006-05-20 18:06:25+00:00   \n",
       "3  ktwarwic <ktwarwic@speedy.uwaterloo.ca>  1999-10-01 00:29:22+00:00   \n",
       "4           email265@gvc.ceas-challenge.cc  2008-08-06 20:56:29+00:00   \n",
       "\n",
       "                                        subject  \\\n",
       "0                                       regalto   \n",
       "1                          CNN.com Daily Top 10   \n",
       "2  Don't waste your time. Cheap pills are here.   \n",
       "3                     her barbour many champion   \n",
       "4                          CNN.com Daily Top 10   \n",
       "\n",
       "                                                body  label  urls  \\\n",
       "0  Men's Products' Discounts!!! 80% off SALE !!! ...    1.0   1.0   \n",
       "1  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...    1.0   1.0   \n",
       "2  Dear customer.Wanna know how to save much on y...    1.0   0.0   \n",
       "3  \\n\\n\\nThis one will explode!\\n\\n\\n\\nTarget sym...    1.0   0.0   \n",
       "4  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...    1.0   1.0   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  regalto Men's Products' Discounts!!! 80% off S...   \n",
       "1  CNN.com Daily Top 10 >+=+=+=+=+=+=+=+=+=+=+=+=...   \n",
       "2  Don't waste your time. Cheap pills are here. D...   \n",
       "3  her barbour many champion \\n\\n\\nThis one will ...   \n",
       "4  CNN.com Daily Top 10 >+=+=+=+=+=+=+=+=+=+=+=+=...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  regalto men product discount sale vaqra pill o...   \n",
       "1  cnncom daily top daily top cnncom top video st...   \n",
       "2  dont waste time cheap pill dear customerwanna ...   \n",
       "3  barbour many champion one explodetarget sym bv...   \n",
       "4  cnncom daily top daily top cnncom top video st...   \n",
       "\n",
       "                                      tfidf_features  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                   word2vec_features  \\\n",
       "0  [0.01425081118941307, 0.03438971936702728, 0.0...   \n",
       "1  [0.0004533067112788558, 0.03454103693366051, -...   \n",
       "2  [0.0284550990909338, 0.02321099303662777, -0.0...   \n",
       "3  [0.036815643310546875, 0.01973724365234375, 0....   \n",
       "4  [0.0004533067112788558, 0.03454103693366051, -...   \n",
       "\n",
       "                                transformer_features  \\\n",
       "0  [-0.09238181263208389, 0.037367917597293854, -...   \n",
       "1  [-0.049507372081279755, -0.06389272958040237, ...   \n",
       "2  [0.021192213520407677, 0.030631644651293755, 0...   \n",
       "3  [-0.03742770478129387, -0.06625784933567047, -...   \n",
       "4  [-0.049507372081279755, -0.06389272958040237, ...   \n",
       "\n",
       "                                     sender_features  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                    subject_features  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                          date_features           body_features  \\\n",
       "0  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  [0, 25, 1.0, 0.0, 0.0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]   [0, 0, 0.0, 0.0, 1.0]   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]   [0, 3, 1.0, 0.0, 0.0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  [0, 15, 1.0, 0.0, 0.0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]   [0, 0, 0.0, 0.0, 1.0]   \n",
       "\n",
       "                                     body_features_2  \n",
       "0  [0, 0, 181, 4.983425414364641, 8, 0.0109190172...  \n",
       "1  [0, 0, 293, 11.542662116040956, 100, 0.0002550...  \n",
       "2  [0, 0, 95, 5.252631578947368, 0, 0.0, 0, 1, 0, 0]  \n",
       "3  [0, 0, 33, 5.363636363636363, 4, 0.05882352941...  \n",
       "4  [0, 0, 293, 11.46075085324232, 100, 0.00025660...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_parquet('train.parquet', index=False)\n",
    "test_dataset.to_parquet('test.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMPLE NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Woon Cher Han\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - accuracy: 0.9130 - loss: 0.2124 - val_accuracy: 0.9730 - val_loss: 0.0730\n",
      "Epoch 2/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9613 - loss: 0.1074 - val_accuracy: 0.9774 - val_loss: 0.0646\n",
      "Epoch 3/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9679 - loss: 0.0904 - val_accuracy: 0.9768 - val_loss: 0.0674\n",
      "Epoch 4/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.9715 - loss: 0.0811 - val_accuracy: 0.9801 - val_loss: 0.0580\n",
      "Epoch 5/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.9752 - loss: 0.0722 - val_accuracy: 0.9805 - val_loss: 0.0562\n",
      "Epoch 6/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.9755 - loss: 0.0710 - val_accuracy: 0.9795 - val_loss: 0.0589\n",
      "Epoch 7/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.9771 - loss: 0.0671 - val_accuracy: 0.9817 - val_loss: 0.0548\n",
      "Epoch 8/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.9793 - loss: 0.0610 - val_accuracy: 0.9813 - val_loss: 0.0543\n",
      "Epoch 9/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0545 - val_accuracy: 0.9822 - val_loss: 0.0522\n",
      "Epoch 10/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.9817 - loss: 0.0542 - val_accuracy: 0.9824 - val_loss: 0.0505\n",
      "Epoch 11/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.9826 - loss: 0.0527 - val_accuracy: 0.9815 - val_loss: 0.0539\n",
      "Epoch 12/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.9827 - loss: 0.0512 - val_accuracy: 0.9830 - val_loss: 0.0501\n",
      "Epoch 13/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9835 - loss: 0.0483 - val_accuracy: 0.9684 - val_loss: 0.2054\n",
      "Epoch 14/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9833 - loss: 0.0471 - val_accuracy: 0.9830 - val_loss: 0.0520\n",
      "Epoch 15/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.9850 - loss: 0.0452 - val_accuracy: 0.9846 - val_loss: 0.0480\n",
      "Epoch 16/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9846 - loss: 0.0441 - val_accuracy: 0.9836 - val_loss: 0.0512\n",
      "Epoch 17/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9842 - loss: 0.0470 - val_accuracy: 0.9841 - val_loss: 0.0476\n",
      "Epoch 18/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9858 - loss: 0.0423 - val_accuracy: 0.9845 - val_loss: 0.0478\n",
      "Epoch 19/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.9849 - loss: 0.0441 - val_accuracy: 0.9843 - val_loss: 0.0505\n",
      "Epoch 20/20\n",
      "\u001b[1m3168/3168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9861 - loss: 0.0400 - val_accuracy: 0.9846 - val_loss: 0.0492\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Accuracy: 0.9857\n",
      "Precision: 0.9825\n",
      "Recall: 0.9868\n",
      "F1 Score: 0.9846\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')\n",
    "\n",
    "# preextracted features\n",
    "def combine_features(df):\n",
    "    sender_features = np.array(df['sender_features'].tolist())\n",
    "    subject_features = np.array(df['subject_features'].tolist())\n",
    "    date_features = np.array(df['date_features'].tolist())\n",
    "    body_features = np.array(df['body_features'].tolist())\n",
    "    tfidf_features = np.array(df['tfidf_features'].tolist())\n",
    "    word2vec_features = np.array(df['word2vec_features'].tolist())\n",
    "    transformer_features = np.array(df['word2vec_features'].tolist())\n",
    "    return np.concatenate([sender_features, subject_features, date_features, body_features, word2vec_features, tfidf_features, transformer_features], axis=1)\n",
    "\n",
    "\n",
    "# train data\n",
    "X = combine_features(train)\n",
    "y = train['label'].values\n",
    "\n",
    "# split 80 20\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_val, y_val), \n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# prepare test data\n",
    "X_test = combine_features(test)\n",
    "\n",
    "# predict test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary labels (0 or 1)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "y_true = test['label'].values\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "precision = precision_score(y_true, y_pred_binary)\n",
    "recall = recall_score(y_true, y_pred_binary)\n",
    "f1 = f1_score(y_true, y_pred_binary)\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM WITH FEEDFORWARD NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101360, 50)\n",
      "(101360, 1428)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Woon Cher Han\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Woon Cher Han\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_80', 'keras_tensor_83']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1584/1584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 310ms/step - accuracy: 0.8994 - loss: 1.0963 - val_accuracy: 0.9794 - val_loss: 0.1207\n",
      "Epoch 2/10\n",
      "\u001b[1m1584/1584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m516s\u001b[0m 326ms/step - accuracy: 0.9788 - loss: 0.1143 - val_accuracy: 0.9798 - val_loss: 0.0952\n",
      "Epoch 3/10\n",
      "\u001b[1m1584/1584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 316ms/step - accuracy: 0.9878 - loss: 0.0748 - val_accuracy: 0.9796 - val_loss: 0.0983\n",
      "Epoch 4/10\n",
      "\u001b[1m1584/1584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 317ms/step - accuracy: 0.9874 - loss: 0.0747 - val_accuracy: 0.9781 - val_loss: 0.1208\n",
      "Epoch 5/10\n",
      "\u001b[1m1584/1584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 312ms/step - accuracy: 0.9911 - loss: 0.0569 - val_accuracy: 0.9738 - val_loss: 0.1344\n",
      "\n",
      "Training complete.\n",
      "Best Epoch - Training Loss: 0.0605, Validation Loss: 0.0952, Training Accuracy: 0.9905, Validation Accuracy: 0.9798\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step\n",
      "Accuracy: 0.9821\n",
      "Precision: 0.9842\n",
      "Recall: 0.9773\n",
      "F1 Score: 0.9807\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50  \n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# preextracted features\n",
    "def combine_features(df):\n",
    "    sender_features = np.array(df['sender_features'].tolist())\n",
    "    subject_features = np.array(df['subject_features'].tolist())\n",
    "    date_features = np.array(df['date_features'].tolist())\n",
    "    body_features = np.array(df['body_features'].tolist())\n",
    "    tfidf_features = np.array(df['tfidf_features'].tolist())\n",
    "    word2vec_features = np.array(df['word2vec_features'].tolist())\n",
    "    return np.concatenate([sender_features, subject_features, date_features, body_features, tfidf_features ,word2vec_features ], axis=1)\n",
    "\n",
    "# tokenise text, and do padding so that length is equal\n",
    "def preprocess_text(df, max_sequence_length, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(df['processed_text'])\n",
    "    text_sequences = tokenizer.texts_to_sequences(df['processed_text'])\n",
    "    text_padded = pad_sequences(text_sequences, maxlen=max_sequence_length)\n",
    "    return text_padded, tokenizer\n",
    "\n",
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')\n",
    "\n",
    "X_train_combined = combine_features(train)\n",
    "X_test_combined = combine_features(test)\n",
    "\n",
    "# preprocess text for lstm\n",
    "X_train_text, tokenizer = preprocess_text(train, MAX_SEQUENCE_LENGTH)\n",
    "X_test_text, _ = preprocess_text(test, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "y_train = train['label'].values\n",
    "y_test = test['label'].values\n",
    "\n",
    "# split 80 20\n",
    "X_train, X_val, y_train, y_val, body_train, body_val = train_test_split(\n",
    "    X_train_combined, y_train, X_train_text, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(body_train.shape)  \n",
    "print(X_train.shape)     \n",
    "\n",
    "# lstm\n",
    "input_body = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(input_body)\n",
    "lstm_out = LSTM(32)(embedding_layer)  \n",
    "\n",
    "# Define input for other features\n",
    "input_features = Input(shape=(X_train_combined.shape[1],))  \n",
    "concat = Concatenate()([lstm_out, input_features])\n",
    "\n",
    "# feed forward with batch normalisation, dropout, relu and l2 regulatization\n",
    "dense_out = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(concat)  \n",
    "dense_out = BatchNormalization()(dense_out)\n",
    "dense_out = Dropout(0.5)(dense_out)\n",
    "dense_out = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(dense_out) \n",
    "dense_out = BatchNormalization()(dense_out)\n",
    "dense_out = Dropout(0.3)(dense_out)\n",
    "dense_out = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(dense_out) \n",
    "dense_out = BatchNormalization()(dense_out)\n",
    "dense_out = Dropout(0.3)(dense_out)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dense_out)\n",
    "\n",
    "model = Model(inputs=[input_body, input_features], outputs=output)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    [body_train, X_train], \n",
    "    y_train, \n",
    "    epochs=10,  \n",
    "    batch_size=64, \n",
    "    validation_data=([body_val, X_val], y_val),\n",
    "    callbacks=[early_stopping],  \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(f\"Best Epoch - Training Loss: {min(history.history['loss']):.4f}, \"\n",
    "      f\"Validation Loss: {min(history.history['val_loss']):.4f}, \"\n",
    "      f\"Training Accuracy: {max(history.history['accuracy']):.4f}, \"\n",
    "      f\"Validation Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "# predict test set\n",
    "y_pred = model.predict([X_test_text, X_test_combined])\n",
    "\n",
    "\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM and CNN Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Woon Cher Han\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Woon Cher Han\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_95', 'keras_tensor_102']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1584/1584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 326ms/step - accuracy: 0.8957 - loss: 1.0651 - val_accuracy: 0.9618 - val_loss: 0.1726\n",
      "Epoch 2/10\n",
      "\u001b[1m1584/1584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 318ms/step - accuracy: 0.9791 - loss: 0.1155 - val_accuracy: 0.9777 - val_loss: 0.1011\n",
      "Epoch 3/10\n",
      "\u001b[1m1584/1584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 318ms/step - accuracy: 0.9862 - loss: 0.0822 - val_accuracy: 0.9820 - val_loss: 0.0879\n",
      "Epoch 4/10\n",
      "\u001b[1m1584/1584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 339ms/step - accuracy: 0.9895 - loss: 0.0610 - val_accuracy: 0.9781 - val_loss: 0.0971\n",
      "Epoch 5/10\n",
      "\u001b[1m1584/1584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 322ms/step - accuracy: 0.9926 - loss: 0.0473 - val_accuracy: 0.9764 - val_loss: 0.1090\n",
      "Epoch 6/10\n",
      "\u001b[1m1584/1584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 316ms/step - accuracy: 0.9932 - loss: 0.0432 - val_accuracy: 0.9738 - val_loss: 0.1488\n",
      "\n",
      "Training complete.\n",
      "Best Epoch - Training Loss: 0.0435, Validation Loss: 0.0879, Training Accuracy: 0.9931, Validation Accuracy: 0.9820\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step\n",
      "Accuracy: 0.9822\n",
      "Precision: 0.9828\n",
      "Recall: 0.9790\n",
      "F1 Score: 0.9809\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Conv1D, MaxPooling1D, Flatten, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50  \n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# preextracted features\n",
    "def combine_features(df):\n",
    "    sender_features = np.array(df['sender_features'].tolist())\n",
    "    subject_features = np.array(df['subject_features'].tolist())\n",
    "    date_features = np.array(df['date_features'].tolist())\n",
    "    body_features = np.array(df['body_features'].tolist())\n",
    "    tfidf_features = np.array(df['tfidf_features'].tolist())\n",
    "    word2vec_features = np.array(df['word2vec_features'].tolist())\n",
    "    return np.concatenate([sender_features, subject_features, date_features, body_features, tfidf_features, word2vec_features], axis=1)\n",
    "\n",
    "# tokenise and padding \n",
    "def preprocess_text(df, max_sequence_length, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(df['processed_text'])\n",
    "    text_sequences = tokenizer.texts_to_sequences(df['processed_text'])\n",
    "    text_padded = pad_sequences(text_sequences, maxlen=max_sequence_length)\n",
    "    return text_padded, tokenizer\n",
    "\n",
    "\n",
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')\n",
    "\n",
    "X_train_combined = combine_features(train)\n",
    "X_test_combined = combine_features(test)\n",
    "\n",
    "# for cnn input\n",
    "X_train_text, tokenizer = preprocess_text(train, MAX_SEQUENCE_LENGTH)\n",
    "X_test_text, _ = preprocess_text(test, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "y_train = train['label'].values\n",
    "y_test = test['label'].values\n",
    "\n",
    "# split 80 20\n",
    "X_train, X_val, y_train, y_val, body_train, body_val = train_test_split(\n",
    "    X_train_combined, y_train, X_train_text, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "input_body = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(input_body)\n",
    "\n",
    "# CNN layers\n",
    "conv1 = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding_layer)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "\n",
    "conv2 = Conv1D(filters=128, kernel_size=3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "\n",
    "# LSTM layer after CNN\n",
    "lstm_out = LSTM(32)(pool2)\n",
    "\n",
    "\n",
    "input_features = Input(shape=(X_train_combined.shape[1],))  \n",
    "\n",
    "# concatenate CNN-LSTM output with other features\n",
    "concat = Concatenate()([lstm_out, input_features])\n",
    "\n",
    "# FC layers\n",
    "dense_out = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(concat)  # L2 Regularization\n",
    "dense_out = BatchNormalization()(dense_out)\n",
    "dense_out = Dropout(0.5)(dense_out)\n",
    "dense_out = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(dense_out)  # L2 Regularization\n",
    "dense_out = BatchNormalization()(dense_out)\n",
    "dense_out = Dropout(0.3)(dense_out)\n",
    "dense_out = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(dense_out)  # L2 Regularization\n",
    "dense_out = BatchNormalization()(dense_out)\n",
    "dense_out = Dropout(0.3)(dense_out)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dense_out)\n",
    "\n",
    "model = Model(inputs=[input_body, input_features], outputs=output)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [body_train, X_train],  \n",
    "    y_train, \n",
    "    epochs=10,  \n",
    "    batch_size=64, \n",
    "    validation_data=([body_val, X_val], y_val),\n",
    "    callbacks=[early_stopping],  \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(f\"Best Epoch - Training Loss: {min(history.history['loss']):.4f}, \"\n",
    "      f\"Validation Loss: {min(history.history['val_loss']):.4f}, \"\n",
    "      f\"Training Accuracy: {max(history.history['accuracy']):.4f}, \"\n",
    "      f\"Validation Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# predict on test\n",
    "y_pred = model.predict([X_test_text, X_test_combined])\n",
    "\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
